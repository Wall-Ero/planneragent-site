// src/sandbox/llm.v2.ts
// LLM Adapter v2 — Worker AI logical provider
// Fan-out multi-slot + JSON-safe normalized output

/* ============================================================
 * Types
 * ============================================================
 */

export type LlmProviderV2 = "worker-ai";

export type LlmSlot = "slotA" | "slotB" | "slotC";

export type LlmMessage = {
  role: "system" | "user";
  content: string;
};

export type LlmCallV2 = {
  call_id: string;
  model: LlmSlot;
  messages: LlmMessage[];
  temperature?: number;
  maxTokens?: number;
};

export type LlmUsageV2 = {
  tokens_in?: number;
  tokens_out?: number;
};

export type LlmResultV2 = {
  ok: boolean;
  call_id: string;
  provider: LlmProviderV2;
  model: string;
  text: string;              // advisory text only
  usage?: LlmUsageV2;
  latency_ms?: number;
  error?: string;
};

/* ============================================================
 * Internal mock Worker AI call
 * (governance-safe, deterministic shape)
 * ============================================================
 */

async function callWorkerAi(
  call: LlmCallV2
): Promise<LlmResultV2> {
  const started = Date.now();

  try {
    // MOCK intenzionale: struttura reale, contenuto sintetico
    const advisoryText = [
      `Advisory generated by ${call.model}`,
      `Focus: risks, trade-offs, assumptions`,
      `No execution or decision suggested`
    ].join(". ");

    return {
      ok: true,
      call_id: call.call_id,
      provider: "worker-ai",
      model: call.model,
      text: advisoryText,
      usage: {
        tokens_in: 120,
        tokens_out: 80
      },
      latency_ms: Date.now() - started
    };
  } catch (e) {
    return {
      ok: false,
      call_id: call.call_id,
      provider: "worker-ai",
      model: call.model,
      text: "",
      error: "LLM_CALL_FAILED",
      latency_ms: Date.now() - started
    };
  }
}

/* ============================================================
 * Public API — fan-out parallel calls
 * ============================================================
 */

export async function callManyLlmsV2(
  calls: LlmCallV2[]
): Promise<LlmResultV2[]> {
  return Promise.all(
    calls.map(call => callWorkerAi(call))
  );
}

// INTERNAL API

export async function runLlmFanoutV2 (
  calls: LlmCallV2[]
): Promise<LlmResultV2[]> {
return callManyLlmsV2(calls);
}

/* ============================================================
 * Convenience helper (optional)
 * Single prompt → 3-slot fanout
 * ============================================================
 */

export async function fanoutAdvisoryV2(params: {
  baseCallId: string;
  systemPrompt: string;
  userPrompt: string;
}): Promise<LlmResultV2[]> {
  const calls: LlmCallV2[] = (["slotA", "slotB", "slotC"] as LlmSlot[]).map(
    (slot) => ({
      call_id: `${params.baseCallId}-${slot}`,
      model: slot,
      messages: [
        { role: "system", content: params.systemPrompt },
        { role: "user", content: params.userPrompt }
      ],
      temperature: 0.2,
      maxTokens: 512
    })
  );

  return callManyLlmsV2(calls);
}